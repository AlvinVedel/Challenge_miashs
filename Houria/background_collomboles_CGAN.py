# -*- coding: utf-8 -*-
"""Background collomboles test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rI164HQ5ea6zBAOo4Nj287A2TOADSR6U
# Libraries
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import xml.etree.ElementTree as ET
import glob
from IPython.display import Image, display
import matplotlib
import matplotlib.pyplot as plt
import random
from tensorflow.keras.applications.resnet import preprocess_input
import tensorflow.keras as keras
from tensorflow.keras.models import load_model, Model
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.layers import Add,Concatenate, MaxPooling2D,Activation, Input, Lambda, Conv2D, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, Conv2DTranspose
from tensorflow.keras  import backend as K
from scipy import misc
from PIL import Image
import os

os.environ["CUDA_VISIBLE_DEVICES"] = '0'

"""# Récupérer les images crop du train et le label associé"""

labels_df = pd.read_csv('/home/barrage/grp3/crops/raw_crops/labels.csv')
labels_df

X_train = []
Y_train = []

processed_images = {}

folder_path = '/home/barrage/grp3/crops/raw_crops/'

for index, row in labels_df.iterrows():
    image_name = row['img_name']
    labels = [row['label1'], row['label2'], row['label3'], row['label4']]

    if image_name in processed_images:
        continue

    image_path = os.path.join(folder_path, image_name)
    image = Image.open(image_path)
    image_np = np.array(image)

    chosen_label = labels[0]

    X_train.append(image_np)
    Y_train.append(int(chosen_label))

    processed_images[image_name] = True

#print(len(X_train))

import matplotlib.pyplot as plt

random_image = X_train[1]
random_label = Y_train[1]

plt.imshow(random_image)
plt.title(f'Label: {random_label}')
plt.axis('off')
plt.show()

image_save_folder = "/home/barrage/grp3/datatest/"

X_test = []

for image_name in os.listdir(image_save_folder):
    if image_name.endswith(('.png', '.jpg', '.jpeg')):
        image_path = os.path.join(image_save_folder, image_name)

        image = Image.open(image_path)
        image_np = np.array(image)

        X_test.append(image_np)

#print(len(X_test))

import matplotlib.pyplot as plt

random_image = X_test[1]

plt.imshow(random_image)
plt.axis('off')
plt.show()

"""# CGAN"""

import matplotlib.pyplot as plt

# Liste des tailles des images (hauteur, largeur)
image_sizes = [(img.shape[0], img.shape[1]) for img in X_train]

# Extraire les hauteurs et largeurs
heights = [size[0] for size in image_sizes]
widths = [size[1] for size in image_sizes]

plt.figure(figsize=(10, 6))
plt.scatter(widths, heights, alpha=0.5, color='blue', marker='o')
plt.xlabel('Largeur (pixels)')
plt.ylabel('Hauteur (pixels)')
plt.title('Distribution des tailles des images dans X_train')
plt.show()

max_height = max([img.shape[0] for img in X_train])
max_width = max([img.shape[1] for img in X_train])

#print(f"La plus grande image (train) a une taille de {max_height}x{max_width}")

max_height = max([img.shape[0] for img in X_test])
max_width = max([img.shape[1] for img in X_test])

#print(f"La plus grande image (train) a une taille de {max_height}x{max_width}")

batch = 8

def convolution(in_, filter, kernel, strides):
  obj_conv =  keras.layers.Conv2D(filter, kernel,strides=strides, padding='same', use_bias=False)
  x = obj_conv(in_)


  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.LeakyReLU()(x)
  return x

def discriminant():
    inputs = keras.Input(shape=(128, 128, 3))
    x = convolution(inputs, 64, 5, strides=1)
    x = convolution(x, 64, 3, strides=2)
    x = convolution(x, 64*2, 3, strides=1)
    x = convolution(x, 64*2, 3, strides=2)
    x = convolution(x, 64*2*2, 3, strides=1)
    x = convolution(x, 64*2*2, 3, strides=2)
    x = convolution(x, 512, 3, strides=1)
    x = keras.layers.Conv2D(1, 3,strides=1, padding='same', use_bias=True)(x)

    return keras.Model(inputs=inputs, outputs=x)


def res(in_):
  x = convolution(in_, in_.shape[-1], 3, 1)
  x = keras.layers.Conv2D(in_.shape[-1], 3,strides=1, padding='same', use_bias=False)(x)
  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.add([x, in_])
  return x

def generateur():
  inputs = keras.Input(shape=(128, 128, 3))
  x = convolution(inputs,64,5,1)
  x = toto1 = convolution(x,64,3,1)#96
  x = convolution(x,64*2,3,2)#96/2
  x = toto2 = convolution(x,64*2,3,1)#96/2
  x = convolution(x,64*4,3,2)#96/4
  x = convolution(x,64*4,3,1)#96/4

  for _ in range(4):
    x = res(x)#96/4

  x = Conv2DTranspose(64*2, 5, strides=2,padding='same', use_bias=False )(x)#96/2
  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.LeakyReLU()(x)

  x = keras.layers.Concatenate(3)([x, toto2])
  x = convolution(x,64*2,3,1)
  x = convolution(x,64*2,3,1)
  x = Conv2DTranspose(64, 5,strides=2, padding='same', use_bias=False )(x)#96
  x = keras.layers.BatchNormalization()(x)
  x = keras.layers.LeakyReLU()(x)
  x = keras.layers.Concatenate(3)([x, toto1])
  x = convolution(x,64,3,1)
  x = convolution(x,64,3,1)
  x = keras.layers.Conv2D(3,5,strides=1, padding='same', use_bias=True)(x)
  x = keras.activations.sigmoid(x)
  return keras.Model(inputs=inputs, outputs=x)

def classifier():
    inputs = keras.Input(shape=(128, 128, 3))
    x = convolution(inputs, 64, 5, 1)
    x = convolution(x, 32, 3, 2)
    x = convolution(x, 64, 3, 1)
    x = convolution(x, 128, 3, 2)

    x = keras.layers.Flatten()(x)

    x = keras.layers.Dense(128, activation='relu')(x)
    x = keras.layers.Dense(10, activation='softmax')(x)
    
    model = keras.Model(inputs=inputs, outputs=x)
    return model

model = classifier()
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

D_A  = discriminant()
D_B  = discriminant()

G_A2B = generateur()
G_B2A = generateur()

D_optimizer = keras.optimizers.Adam(learning_rate=0.001,beta_1=0.5 )
G_optimizer = keras.optimizers.Adam(learning_rate=0.001,beta_1=0.5 )

def train_D(A, B, A2B, B2A):
    with tf.GradientTape() as t:
        A_d_logits   = D_A(  A, training=True)
        B2A_d_logits = D_A(B2A, training=True)#B2A fake
        A_d_loss = tf.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(A_d_logits), A_d_logits)
        B2A_d_loss = tf.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(B2A_d_logits), B2A_d_logits)

        B_d_logits   = D_B(  B, training=True)
        A2B_d_logits = D_B(A2B, training=True)
        B_d_loss = tf.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(B_d_logits), B_d_logits)
        A2B_d_loss = tf.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(A2B_d_logits), A2B_d_logits)

        D_loss = (A_d_loss + B2A_d_loss) + (B_d_loss + A2B_d_loss)

    D_grad = t.gradient(D_loss, D_A.trainable_variables + D_B.trainable_variables)
    D_optimizer.apply_gradients(zip(D_grad, D_A.trainable_variables + D_B.trainable_variables))

def train_G(A, B):
    with tf.GradientTape() as t:
      A2B   = G_A2B(A, training=True)
      A2B2A = G_B2A(A2B, training=True)

      B2A = G_B2A(B, training=True)
      B2A2B = G_A2B(B2A, training=True)

      A2B_d_logits = D_B(A2B, training=True)
      B2A_d_logits = D_A(B2A, training=True)

      loss_cycle = tf.losses.MeanAbsoluteError()(A, A2B2A) +  tf.losses.MeanAbsoluteError()(B, B2A2B)
      dis_loss = tf.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(A2B_d_logits), A2B_d_logits) \
                 + tf.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(B2A_d_logits), B2A_d_logits)

      """
      iden_loss = identity_loss_fn(A, A2B) + identity_loss_fn(B, B2A)#1er solution

      A2A = G_B2A(A, training=True)
      B2B = G_A2B(B, training=True)
      iden_loss = identity_loss_fn(A, A2A) + identity_loss_fn(B, B2B)#2eme solution
      """
      g_loss = 10 * loss_cycle + dis_loss
    G_grad = t.gradient(g_loss, G_A2B.trainable_variables + G_B2A.trainable_variables)
    G_optimizer.apply_gradients(zip(G_grad, G_A2B.trainable_variables + G_B2A.trainable_variables))
    return A2B, B2A

def train_step(A, B):
    A2B, B2A = train_G(A, B)
    train_D(A, B, A2B, B2A)

class DataGenerator(tf.keras.utils.Sequence):
    def __init__(self, imgs, labels, batch_size=8, shuffle=True, target_size=(128, 128)):
        self.batch_size = batch_size
        self.target_size = target_size 
        self.data = imgs
        self.labels = labels
        self.indices = list(range(0, len(self.data)))
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        return len(self.indices) // self.batch_size

    def __getitem__(self, index):
        # Sélectionner un batch d'indices
        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]
        keys = [self.indices[k] for k in batch_indices]
        return self.__get_data(keys)

    def on_epoch_end(self):
        # Re-mélanger les indices à la fin de chaque époque
        self.indices = np.arange(len(self.data))
        if self.shuffle:
            np.random.shuffle(self.indices)

    def __get_data(self, keys):
        X = np.empty((self.batch_size, *self.target_size, 3))
        Y = np.empty(self.batch_size)
        for i, id in enumerate(keys):
            image = self.data[id]
            image_resized = tf.image.resize(image, self.target_size)
            X[i] = image_resized
            Y[i] = int(self.labels[id])

        return {'img': X, 'labels':Y}
    

batch_size = 8

database_train  = DataGenerator(imgs = X_train[int(1*batch_size):], labels=Y_train[int(1*batch_size):],  batch_size=int(batch_size))
database_train_val  = DataGenerator(imgs = X_train[:int(1*batch_size)],    labels=Y_train[:int(1*batch_size)],  batch_size=int(batch_size))

database_test = DataGenerator(X_test[int(1*batch_size):], labels=np.full(len(X_test[int(1*batch_size):]), -1), batch_size=int(batch_size))
database_test_val = DataGenerator(X_test[:int(1*batch_size)], labels=np.full(len(X_test[:int(1*batch_size)]), -1), batch_size=int(batch_size))

print ("Taille batch database_train : ", database_train.__len__())
print ("Taille batch database_test : ", database_test.__len__())
print ("Taille batch database_train val : ", database_train_val.__len__())
print ("Taille batch database_test val: ", database_test_val.__len__())

At = database_train_val.__getitem__(0)['labels']
print(At)

ib = 0
ia = 0
for ite in range(20000):
    B = database_test.__getitem__(ib)['img']
    print(B.shape)
    A = database_train.__getitem__(ia)['img']
    print(A.shape)
    label_A = database_train.__getitem__(ia)['labels']

    # Mise à jour des indices du générateur
    if ite % 10 == 0:
        ib = (ib + 1) % database_test.__len__()
        ia = (ia + 1) % database_train.__len__()

        if ib == 0:
            database_test.on_epoch_end()
        if ia == 0:
            database_train.on_epoch_end()

    train_step(A, B)

    A2B = G_A2B(A, training=False)
    B2A = G_B2A(B, training=False)

    label_A_pred = model.predict(A)
    label_A2B_pred = model.predict(A2B)

    # Comparaison avec le label réel de A
    correct_A_pred = np.argmax(label_A_pred) == label_A
    correct_A2B_pred = np.argmax(label_A2B_pred) == label_A

    if ite % 100 == 0:
        print(f"Iteration {ite}:")
        print(f"Label de A prédit : {np.argmax(label_A_pred)}, réel : {label_A}, Correct ? {correct_A_pred}")
        print(f"Label de A2B prédit : {np.argmax(label_A2B_pred)}, réel : {label_A}, Correct ? {correct_A2B_pred}")

        plt.figure(dpi=100, figsize=[12, 8])
        for i in range(4):
            plt.subplot(4, 4, 1 + i * 4)
            plt.imshow(A[i])
            plt.subplot(4, 4, 2 + i * 4)
            plt.imshow(A2B[i])
            plt.subplot(4, 4, 3 + i * 4)
            plt.imshow(B[i])
            plt.subplot(4, 4, 4 + i * 4)
            plt.imshow(B2A[i])
        plt.show()